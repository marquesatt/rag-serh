import os
import json
import uuid
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from dotenv import load_dotenv
from typing import Optional
from vertexai.generative_models import (
    SafetySetting, HarmCategory, HarmBlockThreshold,
    Content, Part
)

load_dotenv()

PROJECT_ID = os.getenv("PROJECT_ID", "serhrag")
LOCATION = os.getenv("LOCATION", "europe-west4")
PORT = int(os.getenv("PORT", 8000))

# Configura credenciais do Google Cloud de forma segura
def setup_google_credentials():
    """Setup Google Cloud credentials usando vari√°vel de ambiente"""
    import tempfile
    
    # 1. Tenta usar vari√°vel de ambiente (produ√ß√£o e desenvolvimento)
    creds_json = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON")
    if creds_json:
        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                f.write(creds_json)
                temp_creds_path = f.name
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = temp_creds_path
            print(f"‚úì Credenciais carregadas da vari√°vel de ambiente")
            return
        except Exception as e:
            print(f"‚úó Erro ao processar GOOGLE_APPLICATION_CREDENTIALS_JSON: {e}")
    
    # 2. Tenta arquivo local como fallback (desenvolvimento)
    import glob
    json_files = glob.glob("./serhrag*.json")
    if json_files:
        local_creds = json_files[0]
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = local_creds
        print(f"‚úì Credenciais carregadas do arquivo local: {local_creds}")
        return
    
    print("‚úó Nenhuma credencial encontrada")
    
    # Tenta arquivo local (desenvolvimento)
    local_creds = "./serhrag-d481c39ed083.json"
    if os.path.exists(local_creds):
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = local_creds
        print(f"‚úì Credenciais carregadas do arquivo local")
        return True
    
    print("‚úó Nenhuma credencial encontrada")
    print("   Para Railway: configure GOOGLE_CREDENTIALS_JSON nas vari√°veis de ambiente")
    print("   Para desenvolvimento local: adicione serhrag-d481c39ed083.json")
    return False

setup_google_credentials()

corpus = None
model = None


def init_vertex_ai():
    global corpus, model
    try:
        # imports locais para evitar conflitos
        import vertexai
        from vertexai import rag
        from vertexai.generative_models import GenerativeModel, Tool
        
        # autentica com google cloud
        print(f"Inicializando Vertex AI com PROJECT_ID={PROJECT_ID}, LOCATION={LOCATION}")
        vertexai.init(project=PROJECT_ID, location=LOCATION)
        print(f"‚úì Vertex AI inicializado")
        
        # carrega corpus
        print(f"Listando corpora dispon√≠veis...")
        corpora = list(rag.list_corpora())
        print(f"Corpora encontrados: {len(corpora)}")
        for corpus_item in corpora:
            print(f"  - {corpus_item.display_name} (ID: {corpus_item.name})")
        
        if corpora:
            corpus = corpora[0]
            print(f"‚úì Corpus carregado: {corpus.display_name}")
            
            # configura rag retrieval
            config = rag.RagRetrievalConfig(
                top_k=3,
                filter=rag.Filter(vector_distance_threshold=0.5),
            )
            
            tool = Tool.from_retrieval(
                retrieval=rag.Retrieval(
                    source=rag.VertexRagStore(
                        rag_resources=[rag.RagResource(rag_corpus=corpus.name)],
                        rag_retrieval_config=config,
                    ),
                )
            )
            
            # inicializa modelo com RAG tool
            model = GenerativeModel(
                model_name="gemini-2.0-flash",
                tools=[tool],
                system_instruction="""Voc√™ √© um assistente especializado em SERH (Sistema Eletr√¥nico de Recursos Humanos).

INSTRU√á√ïES CR√çTICAS:
1. SEMPRE busque respostas no corpus do SERH primeiro
2. Responda APENAS baseado no que encontrar no corpus
3. Se a informa√ß√£o n√£o estiver dispon√≠vel no corpus, diga: "Essa informa√ß√£o n√£o est√° dispon√≠vel no sistema SERH"
4. NUNCA invente ou use conhecimento externo - apenas o que est√° no corpus
5. N√£o mencione termos t√©cnicos como "documento", "RAG", "corpus" ou "buscar"

Tom: Profissional, amig√°vel e direto. Respostas naturais e conversacionais."""
            )
            print(f"‚úì Modelo Gemini pronto com RAG tool")
            return True
        else:
            print("‚úó NENHUM CORPUS ENCONTRADO NO GOOGLE CLOUD")
            print(f"   Verifique:")
            print(f"   - Se PROJECT_ID est√° correto: {PROJECT_ID}")
            print(f"   - Se LOCATION est√° correta: {LOCATION}")
            print(f"   - Se o corpus foi criado no Google Cloud RAG")
            return False
    except Exception as e:
        print(f"‚úó Erro ao inicializar: {e}")
        import traceback
        traceback.print_exc()
        return False


@asynccontextmanager
async def lifespan(app: FastAPI):
    print("iniciando api...")
    init_vertex_ai()
    yield
    print("encerrando api...")


app = FastAPI(lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

# Armazena hist√≥rico de conversas em mem√≥ria - CONTROLE MANUAL
# Formato: {conversation_id: [Content(role="user"|"model", parts=[Part(text="...")])]}
conversations = {}


class Message(BaseModel):
    text: str
    conversation_id: Optional[str] = None  # Se None, cria uma nova conversa


class ChatResponse(BaseModel):
    response: str
    conversation_id: str
    corpus: str
    asking_clarification: bool


@app.get("/")
def root():
    return {"api": "rag", "version": "1.0.0", "status": "running"}


@app.get("/health")
def health():
    status = "ok" if model and corpus else "initializing"
    return {"status": status, "corpus": corpus.display_name if corpus else None}


@app.get("/conversation/{conversation_id}")
def get_conversation(conversation_id: str):
    """Retorna hist√≥rico completo de uma conversa"""
    if conversation_id not in conversations:
        return {"error": "conversa n√£o encontrada"}, 404
    
    history = conversations[conversation_id]
    history_formatted = [
        {
            "role": content.role,
            "text": content.parts[0].text if content.parts else ""
        }
        for content in history
    ]
    
    return {
        "conversation_id": conversation_id,
        "history": history_formatted,
        "message_count": len(history)
    }


@app.delete("/conversation/{conversation_id}")
def delete_conversation(conversation_id: str):
    """Deleta uma conversa"""
    if conversation_id not in conversations:
        return {"error": "conversa n√£o encontrada"}, 404
    
    del conversations[conversation_id]
    return {"status": "conversa deletada", "conversation_id": conversation_id}


@app.get("/conversations")
def list_conversations():
    """Lista todas as conversas ativas"""
    return {
        "total_conversations": len(conversations),
        "conversations": [
            {
                "conversation_id": cid,
                "message_count": len(history)
            }
            for cid, history in conversations.items()
        ]
    }


@app.post("/chat")
def chat(msg: Message):
    if not model or not corpus:
        return {"error": "corpus nao carregado"}, 503
    
    if not msg.text.strip():
        return {"error": "mensagem vazia"}, 400
    
    try:
        # Cria ou obt√©m conversa
        conversation_id = msg.conversation_id or str(uuid.uuid4())
        
        if conversation_id not in conversations:
            conversations[conversation_id] = []
        
        history = conversations[conversation_id]
        
        # Constr√≥i conversa completa: hist√≥rico + nova mensagem do usu√°rio
        full_conversation = list(history)
        full_conversation.append(Content(role="user", parts=[Part.from_text(msg.text)]))
        
        # DEBUG DETALHADO
        print(f"\n{'='*70}")
        print(f"üîπ CHAT REQUEST - Conversation ID: {conversation_id}")
        print(f"{'='*70}")
        print(f"üìù Hist√≥rico ANTES: {len(history)} items")
        for i, content in enumerate(history):
            text_preview = content.parts[0].text[:60] if content.parts else ""
            print(f"   [{i}] [{content.role.upper()}]: {text_preview}...")
        
        print(f"\nüì• Nova mensagem: {msg.text}")
        
        print(f"\nüì§ CONVERSA COMPLETA sendo enviada ao modelo ({len(full_conversation)} items):")
        for i, content in enumerate(full_conversation):
            text_preview = content.parts[0].text[:60] if content.parts else ""
            role_display = "USER" if content.role == "user" else "MODEL"
            print(f"   [{i}] [{role_display}]: {text_preview}...")
        
        print(f"\n‚è≥ Aguardando resposta do modelo...")
        print(f"{'='*70}\n")
        
        # Chama modelo COM HIST√ìRICO COMPLETO como primeiro argumento
        response = model.generate_content(
            full_conversation,  # ‚úÖ Passa conversa completa (hist√≥rico + nova msg)
            generation_config={
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 1024,
            },
            safety_settings=[
                SafetySetting(
                    category=HarmCategory.HARM_CATEGORY_HARASSMENT,
                    threshold=HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                ),
                SafetySetting(
                    category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                    threshold=HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                ),
                SafetySetting(
                    category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                    threshold=HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                ),
                SafetySetting(
                    category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                    threshold=HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                ),
            ]
        )
        
        response_text = response.text
        
        # ‚úÖ Salva user message no hist√≥rico
        conversations[conversation_id].append(Content(role="user", parts=[Part.from_text(msg.text)]))
        # ‚úÖ Salva model response no hist√≥rico
        conversations[conversation_id].append(Content(role="model", parts=[Part.from_text(response_text)]))
        
        history = conversations[conversation_id]
        
        # DEBUG
        print(f"{'='*70}")
        print(f"‚úÖ RESPONSE RECEIVED")
        print(f"{'='*70}")
        print(f"üì§ Hist√≥rico DEPOIS: {len(history)} items")
        for i, content in enumerate(history):
            text_preview = content.parts[0].text[:60] if content.parts else ""
            role_display = "USER" if content.role == "user" else "MODEL"
            print(f"   [{i}] [{role_display}]: {text_preview}...")
        
        print(f"\nüìã Resposta do modelo ({len(response_text)} chars):")
        print(f"   {response_text[:100]}...")
        print(f"{'='*70}\n")
        
        # Detecciona se pediu clarifica√ß√£o
        is_asking_clarification = any(keyword in response_text.lower() for keyword in [
            "esclare√ßa", "clarify", "qual √© exatamente", "qual √© o seu", "voc√™ quer dizer",
            "pode ser mais espec√≠fico", "pode detalhar", "qual delas", "qual op√ß√£o",
            "entendo melhor", "como assim", "quer dizer"
        ])
        
        return {
            "response": response_text, 
            "conversation_id": conversation_id,
            "corpus": corpus.display_name,
            "asking_clarification": is_asking_clarification,
            "history_length": len(history)
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"error": str(e)}, 500


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=PORT)

